{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0dac0cd",
   "metadata": {},
   "source": [
    "# Pedestrians Counting (YOLOv11) — A: Detection, B: Tracking, C: Tracking + Scene Counting\n",
    "\n",
    "This notebook processes a people video with **YOLOv11** and Ultralytics tracking.\n",
    "\n",
    "- **Phase A**: Detection-only → saves **`people_detection.mp4`**.\n",
    "- **Phase B**: Detection + Tracking (IDs) → saves **`people_tracked.mp4`**.\n",
    "- **Phase C**: Detection + Tracking + Scene Counting (current) → saves **`people_counted.mp4`**.\n",
    "\n",
    "> Put your weights `yolo11m.pt` and a sample `Pedestrians.mp4` in the working directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38d8057",
   "metadata": {},
   "source": [
    "## 0) Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install ultralytics opencv-python shapely lapx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e3b67b",
   "metadata": {},
   "source": [
    "## 1) Imports, configuration & helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b44af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config OK: yolo11m.pt people2.mp4\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import math\n",
    "from typing import Tuple\n",
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- Configuration ---\n",
    "WEIGHTS = 'yolo11m.pt'   # YOLOv11 weights path\n",
    "SOURCE  = 'Pedestrians.mp4'   \n",
    "\n",
    "# Sanity checks\n",
    "assert Path(WEIGHTS).exists(), 'If missing weights: place YOLOv11 weights at ./yolo11m.pt'\n",
    "assert Path(SOURCE).exists() or SOURCE == 0, 'If missing Pedestrians.mp4'\n",
    "\n",
    "# COCO class id\n",
    "PERSON_ID = 0\n",
    "\n",
    "\n",
    "print('Config OK:', WEIGHTS, SOURCE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87df888",
   "metadata": {},
   "source": [
    "## 2) PHASE A — Detection-only → `people_detection.mp4`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f051eb",
   "metadata": {},
   "source": [
    "This cell:\n",
    "- runs **Detection** on a people video,\n",
    "- draws person boxes,\n",
    "- and saves the result to **`people_detection.mp4`**.\n",
    "\n",
    "Notes:\n",
    "- `classes=[0]` restricts to the **person** class (COCO ID 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951c9ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: people_detection.mp4 (597 frames)\n"
     ]
    }
   ],
   "source": [
    "# PHASE A — Detect (person only) → people_detection.mp4\n",
    "# Expects these to be defined earlier in the notebook:\n",
    "# WEIGHTS (e.g., 'yolo11m.pt'), SOURCE (e.g., 'people.mp4' or 0), PERSON_ID = 0\n",
    "# Requires: ultralytics, opencv-python\n",
    "\n",
    "# 1) Load model\n",
    "model = YOLO(WEIGHTS)\n",
    "\n",
    "\n",
    "# Stream detection results frame-by-frame (Ultralytics reads SOURCE internally)\n",
    "results = model.predict(\n",
    "    source=SOURCE,\n",
    "    stream=True,            # yields one result per frame\n",
    "    conf=0.35,              # raise to be stricter, lower to see more\n",
    "    iou=0.5,                \n",
    "    imgsz=960,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "writer = None\n",
    "frames = 0\n",
    "\n",
    "for r in results:\n",
    "    # Original BGR frame to draw on\n",
    "    frame = r.orig_img.copy()\n",
    "\n",
    "    # Create the writer on the first frame (match width/height; fixed 30 FPS output)\n",
    "    if writer is None:\n",
    "        h, w = frame.shape[:2]\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        writer = cv2.VideoWriter(\"people_detection.mp4\", fourcc, 30.0, (w, h))\n",
    "\n",
    "    # If detections exist, keep only 'person' and draw boxes + confidence\n",
    "    if r.boxes is not None and len(r.boxes) > 0:\n",
    "        cls = r.boxes.cls.int().cpu().numpy()\n",
    "        msk = (cls == PERSON_ID)\n",
    "        if msk.any():\n",
    "            xyxy  = r.boxes.xyxy.cpu().numpy()[msk]   # [x1,y1,x2,y2]\n",
    "            confs = r.boxes.conf.cpu().numpy()[msk]   # confidence per box\n",
    "            for (x1, y1, x2, y2), cf in zip(xyxy, confs):\n",
    "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, f\"person {cf:.2f}\",\n",
    "                            (int(x1), max(0, int(y1) - 5)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the annotated frame\n",
    "    writer.write(frame)\n",
    "    frames += 1\n",
    "\n",
    "# 3) Clean up\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "\n",
    "print(f\"Saved: people_detection.mp4 ({frames} frames)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8c803",
   "metadata": {},
   "source": [
    "## 3) PHASE B — Detection + Tracking (IDs only) → `people_tracked.mp4`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b390a2",
   "metadata": {},
   "source": [
    "This cell:\n",
    "- runs **tracking** (BoT-SORT) on a people video,\n",
    "- draws person boxes + IDs,\n",
    "- and saves the result to **`people_tracked.mp4`**.\n",
    "\n",
    "Notes:\n",
    "- `classes=[0]` restricts to the **person** class (COCO ID 0).\n",
    "- `tracker=\"botsort.yaml\"` reduces ID switches by using bot sort tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29bae775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: people_tracked.mp4\n"
     ]
    }
   ],
   "source": [
    "# PHASE B — Detect + Track (IDs only) → people_tracked.mp4\n",
    "# Expects these to be defined earlier in the notebook:\n",
    "# WEIGHTS (e.g., 'yolo11m.pt'), SOURCE (e.g., 'people.mp4' or 0), PERSON_ID = 0\n",
    "# Requires: ultralytics, opencv-python\n",
    "\n",
    "\n",
    "# Inference params\n",
    "# Start tracking stream:\n",
    "# - BoT-SORT for fewer ID switches\n",
    "# - Only 'person' class\n",
    "# - imgsz 960 = more stable than 640 (but slower)\n",
    "results = model.track(\n",
    "    source=SOURCE,\n",
    "    tracker=\"botsort.yaml\",\n",
    "    stream=True,            # yields one result per frame\n",
    "    classes=[PERSON_ID],    # filter to people\n",
    "    conf=0.35,              # raise to be stricter, lower to see more\n",
    "    iou=0.5,\n",
    "    imgsz=960,\n",
    "    persist=True,           # keep tracker state across frames\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "writer = None               \n",
    "frames = 0\n",
    "\n",
    "for r in results:\n",
    "    # Get the current frame (BGR) to draw on\n",
    "    frame = r.orig_img.copy()\n",
    "\n",
    "    # Create the video writer on the first frame (match input width/height)\n",
    "    if writer is None:\n",
    "        h, w = frame.shape[:2]\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        writer = cv2.VideoWriter(\"people_tracked.mp4\", fourcc, 30.0, (w, h))  # fixed 30 FPS output\n",
    "\n",
    "    # If we have detections with IDs, draw boxes + ID labels\n",
    "    if r.boxes is not None and len(r.boxes) > 0 and getattr(r.boxes, \"id\", None) is not None:\n",
    "        ids = r.boxes.id.int().cpu().numpy()       # tracker IDs (one per detection)\n",
    "        cls = r.boxes.cls.int().cpu().numpy()      # class index per detection\n",
    "        msk = (cls == PERSON_ID)                   # keep only 'person'\n",
    "        xyxy = r.boxes.xyxy.cpu().numpy()[msk]     # boxes for people only\n",
    "        ids_f = ids[msk]                           # matching IDs for people\n",
    "\n",
    "        # Draw each person's box and ID above it\n",
    "        for (x1, y1, x2, y2), tid in zip(xyxy, ids_f):\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 200, 0), 2)\n",
    "            cv2.putText(frame, f\"ID {int(tid)}\",\n",
    "                        (int(x1), max(0, int(y1) - 5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 200, 0), 2)\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    writer.write(frame)\n",
    "\n",
    "# Close the file cleanly\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "\n",
    "print(\"Saved: people_tracked.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1410724",
   "metadata": {},
   "source": [
    "## 4) PHASE C — Detection + Tracking + Scene Counting → `people_counted.mp4`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3316366e",
   "metadata": {},
   "source": [
    "This cell:\n",
    "- runs **tracking** (BoT-SORT) on a people video,\n",
    "- draws person boxes + IDs,\n",
    "- overlays the **current number of people in the scene**,\n",
    "- and saves the result to **`people_counted.mp4`**.\n",
    "\n",
    "Notes:\n",
    "- `classes=[0]` restricts to the **person** class (COCO ID 0).\n",
    "- `tracker=\"botsort.yaml\"` reduces ID switches by using bot sort tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55118902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: people_counted.mp4\n"
     ]
    }
   ],
   "source": [
    "# PHASE C — Detection + Tracking + Scene Counting (current only) → people_counted.mp4\n",
    "# Expects these to be defined earlier in the notebook:\n",
    "# WEIGHTS (e.g., 'yolo11m.pt'), SOURCE (e.g., 'people.mp4' or 0), PERSON_ID = 0\n",
    "# Requires: ultralytics, opencv-python\n",
    "\n",
    "\n",
    "results = model.track(\n",
    "    source=SOURCE,\n",
    "    tracker=\"botsort.yaml\",\n",
    "    stream=True,            # yields one result per frame\n",
    "    classes=[PERSON_ID],    # filter to people\n",
    "    conf=0.35,              # raise to be stricter, lower to see more\n",
    "    iou=0.5,\n",
    "    imgsz=960,\n",
    "    persist=True,           # keep tracker state across frames\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "writer = None\n",
    "frames = 0\n",
    "\n",
    "for r in results:\n",
    "    frame = r.orig_img.copy()\n",
    "\n",
    "    # Create the video writer on the first frame\n",
    "    if writer is None:\n",
    "        h, w = frame.shape[:2]\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        writer = cv2.VideoWriter(\"people_counted.mp4\", fourcc, 30.0, (w, h))  # 30 FPS output\n",
    "\n",
    "    # Collect IDs for people in this frame\n",
    "    current_ids = []\n",
    "    if r.boxes is not None and len(r.boxes) > 0 and getattr(r.boxes, \"id\", None) is not None:\n",
    "        ids = r.boxes.id.int().cpu().numpy()\n",
    "        cls = r.boxes.cls.int().cpu().numpy()\n",
    "        msk = (cls == PERSON_ID)\n",
    "        current_ids = ids[msk].tolist()\n",
    "\n",
    "        # draw boxes + IDs just for clarity\n",
    "        xyxy = r.boxes.xyxy.cpu().numpy()[msk]\n",
    "        for (x1, y1, x2, y2), tid in zip(xyxy, current_ids):\n",
    "            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (0, 170, 255), 2)\n",
    "            cv2.putText(frame, f\"ID {int(tid)}\",\n",
    "                        (int(x1), max(0, int(y1) - 5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 170, 255), 2)\n",
    "\n",
    "    # Show current count\n",
    "    current_count = len(set(current_ids))\n",
    "    cv2.putText(frame, f\"Current in scene: {current_count}\",\n",
    "                (15, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (50, 255, 50), 2)\n",
    "\n",
    "    writer.write(frame)\n",
    "\n",
    "if writer is not None:\n",
    "    writer.release()\n",
    "\n",
    "print(\"Saved: people_counted.mp4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MVclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
